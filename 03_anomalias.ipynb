{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detecção de Anomalias com Isolation Forest\n",
        "## Inventory Anomaly Detector\n",
        "\n",
        "Este notebook realiza a detecção de anomalias em consumo e estoque usando Isolation Forest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Adicionar src ao path\n",
        "sys.path.insert(0, str(Path.cwd()))\n",
        "\n",
        "from src.data_loader import load_inventory_data, load_raw_consumo\n",
        "from src.data_cleaning import clean_consumo, save_processed\n",
        "from src.data_aggregator import aggregate_daily_by_item\n",
        "from src.anomalies import (\n",
        "    train_isolation_forest,\n",
        "    detect_anomalies,\n",
        "    detect_anomalies_consumo_estoque,\n",
        "    save_anomaly_model,\n",
        "    load_anomaly_model\n",
        ")\n",
        "from src.alerts import (\n",
        "    format_anomaly_alert,\n",
        "    format_anomaly_email_html,\n",
        "    send_anomaly_alerts,\n",
        "    send_anomaly_alert_by_product\n",
        ")\n",
        "\n",
        "# Configurações\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Carregar e Preparar Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregar dados de estoque e consumo\n",
        "df = load_inventory_data()\n",
        "print(f\"Shape do dataset: {df.shape}\")\n",
        "print(f\"\\nColunas: {df.columns.tolist()}\")\n",
        "print(f\"\\nPrimeiras linhas:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar informações básicas\n",
        "print(\"Informações do dataset:\")\n",
        "print(df.info())\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Estatísticas descritivas:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Agregação Diária por Item\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agregar dados diários por produto\n",
        "df_aggregated = aggregate_daily_by_item(df)\n",
        "\n",
        "print(f\"\\nShape após agregação: {df_aggregated.shape}\")\n",
        "print(f\"\\nColunas criadas: {df_aggregated.columns.tolist()}\")\n",
        "df_aggregated.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar produtos únicos e período\n",
        "print(f\"Produtos únicos: {df_aggregated['produto_id'].nunique()}\")\n",
        "print(f\"Período: {df_aggregated['data'].min()} a {df_aggregated['data'].max()}\")\n",
        "print(f\"Total de dias: {(df_aggregated['data'].max() - df_aggregated['data'].min()).days + 1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Treinar Isolation Forest\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir features para detecção de anomalias\n",
        "# Usaremos consumo e estoque médios\n",
        "feature_columns = ['consumo_mean', 'estoque_mean']\n",
        "\n",
        "# Verificar se as colunas existem\n",
        "missing_cols = [col for col in feature_columns if col not in df_aggregated.columns]\n",
        "if missing_cols:\n",
        "    print(f\"Aviso: Colunas não encontradas: {missing_cols}\")\n",
        "    print(f\"Colunas disponíveis: {df_aggregated.columns.tolist()}\")\n",
        "else:\n",
        "    print(f\"Features selecionadas: {feature_columns}\")\n",
        "    print(f\"\\nEstatísticas das features:\")\n",
        "    print(df_aggregated[feature_columns].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Treinar modelo Isolation Forest\n",
        "# contamination: proporção esperada de anomalias (10% = 0.1)\n",
        "anomaly_model = train_isolation_forest(\n",
        "    df_aggregated,\n",
        "    feature_columns=feature_columns,\n",
        "    contamination=0.1,  # Esperamos ~10% de anomalias\n",
        "    random_state=42,\n",
        "    n_estimators=100\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Detectar Anomalias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detectar anomalias usando o modelo treinado\n",
        "df_with_anomalies = detect_anomalies(\n",
        "    anomaly_model,\n",
        "    df_aggregated,\n",
        "    feature_columns=feature_columns\n",
        ")\n",
        "\n",
        "print(f\"\\nPrimeiras linhas com anomalias detectadas:\")\n",
        "df_with_anomalies.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filtrar apenas as anomalias\n",
        "anomalies = df_with_anomalies[df_with_anomalies['is_anomaly'] == True].copy()\n",
        "\n",
        "print(f\"Total de anomalias detectadas: {len(anomalies)}\")\n",
        "print(f\"Percentual de anomalias: {len(anomalies)/len(df_with_anomalies)*100:.2f}%\")\n",
        "\n",
        "if len(anomalies) > 0:\n",
        "    print(f\"\\nAnomalias detectadas:\")\n",
        "    print(anomalies[['produto_id', 'data', 'consumo_mean', 'estoque_mean', 'anomaly_score']].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Análise Estatística das Anomalias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estatísticas das anomalias\n",
        "if len(anomalies) > 0:\n",
        "    print(\"Estatísticas das anomalias:\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Score médio: {anomalies['anomaly_score'].mean():.4f}\")\n",
        "    print(f\"Score mínimo: {anomalies['anomaly_score'].min():.4f}\")\n",
        "    print(f\"Score máximo: {anomalies['anomaly_score'].max():.4f}\")\n",
        "    print(f\"\\nAnomalias por produto:\")\n",
        "    print(anomalies['produto_id'].value_counts().head(10))\n",
        "    \n",
        "    print(f\"\\nAnomalias por data:\")\n",
        "    anomalies_by_date = anomalies.groupby('data').size().sort_values(ascending=False)\n",
        "    print(anomalies_by_date.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparar estatísticas: anomalias vs. normal\n",
        "print(\"Comparação: Anomalias vs. Dados Normais\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "normal_data = df_with_anomalies[df_with_anomalies['is_anomaly'] == False]\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Normal': [\n",
        "        normal_data['consumo_mean'].mean(),\n",
        "        normal_data['consumo_mean'].std(),\n",
        "        normal_data['estoque_mean'].mean(),\n",
        "        normal_data['estoque_mean'].std()\n",
        "    ],\n",
        "    'Anomalias': [\n",
        "        anomalies['consumo_mean'].mean(),\n",
        "        anomalies['consumo_mean'].std(),\n",
        "        anomalies['estoque_mean'].mean(),\n",
        "        anomalies['estoque_mean'].std()\n",
        "    ]\n",
        "}, index=['Consumo Médio', 'Consumo Std', 'Estoque Médio', 'Estoque Std'])\n",
        "\n",
        "print(comparison)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualização das Anomalias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gráfico 1: Distribuição de scores de anomalia\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Score de anomalia\n",
        "axes[0, 0].hist(df_with_anomalies['anomaly_score'], bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].axvline(df_with_anomalies['anomaly_score'].mean(), color='red', linestyle='--', label='Média')\n",
        "axes[0, 0].set_title('Distribuição de Scores de Anomalia')\n",
        "axes[0, 0].set_xlabel('Anomaly Score')\n",
        "axes[0, 0].set_ylabel('Frequência')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Consumo vs Estoque (todos os dados)\n",
        "scatter = axes[0, 1].scatter(\n",
        "    df_with_anomalies['consumo_mean'],\n",
        "    df_with_anomalies['estoque_mean'],\n",
        "    c=df_with_anomalies['anomaly_score'],\n",
        "    cmap='RdYlGn_r',\n",
        "    alpha=0.6,\n",
        "    s=50\n",
        ")\n",
        "axes[0, 1].scatter(\n",
        "    anomalies['consumo_mean'],\n",
        "    anomalies['estoque_mean'],\n",
        "    color='red',\n",
        "    marker='x',\n",
        "    s=100,\n",
        "    label='Anomalias',\n",
        "    linewidths=2\n",
        ")\n",
        "axes[0, 1].set_title('Consumo vs Estoque (colorido por score)')\n",
        "axes[0, 1].set_xlabel('Consumo Médio')\n",
        "axes[0, 1].set_ylabel('Estoque Médio')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=axes[0, 1], label='Anomaly Score')\n",
        "\n",
        "# Série temporal de consumo com anomalias destacadas\n",
        "if len(anomalies) > 0:\n",
        "    # Pegar um produto como exemplo\n",
        "    produto_exemplo = anomalies['produto_id'].iloc[0]\n",
        "    df_produto = df_with_anomalies[df_with_anomalies['produto_id'] == produto_exemplo].sort_values('data')\n",
        "    anomalias_produto = anomalies[anomalies['produto_id'] == produto_exemplo].sort_values('data')\n",
        "    \n",
        "    axes[1, 0].plot(df_produto['data'], df_produto['consumo_mean'], label='Consumo', alpha=0.7)\n",
        "    axes[1, 0].scatter(anomalias_produto['data'], anomalias_produto['consumo_mean'], \n",
        "                      color='red', marker='x', s=100, label='Anomalias', linewidths=2)\n",
        "    axes[1, 0].set_title(f'Série Temporal - Consumo (Produto: {produto_exemplo})')\n",
        "    axes[1, 0].set_xlabel('Data')\n",
        "    axes[1, 0].set_ylabel('Consumo Médio')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Série temporal de estoque com anomalias destacadas\n",
        "axes[1, 1].plot(df_produto['data'], df_produto['estoque_mean'], label='Estoque', alpha=0.7, color='green')\n",
        "axes[1, 1].scatter(anomalias_produto['data'], anomalias_produto['estoque_mean'], \n",
        "                   color='red', marker='x', s=100, label='Anomalias', linewidths=2)\n",
        "axes[1, 1].set_title(f'Série Temporal - Estoque (Produto: {produto_exemplo})')\n",
        "axes[1, 1].set_xlabel('Data')\n",
        "axes[1, 1].set_ylabel('Estoque Médio')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gráfico 2: Anomalias por produto e por data\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Anomalias por produto\n",
        "if len(anomalies) > 0:\n",
        "    anomalies_by_product = anomalies['produto_id'].value_counts().head(10)\n",
        "    axes[0].barh(range(len(anomalies_by_product)), anomalies_by_product.values)\n",
        "    axes[0].set_yticks(range(len(anomalies_by_product)))\n",
        "    axes[0].set_yticklabels(anomalies_by_product.index)\n",
        "    axes[0].set_title('Top 10 Produtos com Mais Anomalias')\n",
        "    axes[0].set_xlabel('Número de Anomalias')\n",
        "    axes[0].grid(True, alpha=0.3, axis='x')\n",
        "    \n",
        "    # Anomalias por data\n",
        "    anomalies_by_date = anomalies.groupby('data').size().sort_values(ascending=False).head(10)\n",
        "    axes[1].bar(range(len(anomalies_by_date)), anomalies_by_date.values)\n",
        "    axes[1].set_xticks(range(len(anomalies_by_date)))\n",
        "    axes[1].set_xticklabels([d.strftime('%Y-%m-%d') for d in anomalies_by_date.index], rotation=45)\n",
        "    axes[1].set_title('Top 10 Datas com Mais Anomalias')\n",
        "    axes[1].set_ylabel('Número de Anomalias')\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Formatação de Alertas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Formatar mensagem de alerta (texto)\n",
        "if len(anomalies) > 0:\n",
        "    alert_message = format_anomaly_alert(anomalies.head(10), max_anomalies=10)\n",
        "    print(\"Mensagem de alerta formatada:\")\n",
        "    print(\"=\"*60)\n",
        "    print(alert_message)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Formatar mensagem HTML para email\n",
        "if len(anomalies) > 0:\n",
        "    html_message = format_anomaly_email_html(anomalies.head(20))\n",
        "    print(\"HTML gerado (primeiras 500 caracteres):\")\n",
        "    print(\"=\"*60)\n",
        "    print(html_message[:500])\n",
        "    print(\"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Envio de Alertas (Exemplo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de envio de alertas\n",
        "# NOTA: Configure os webhooks/email em src/config.py antes de usar\n",
        "\n",
        "if len(anomalies) > 0:\n",
        "    # Filtrar apenas anomalias com score alto (>= 0.7)\n",
        "    high_score_anomalies = anomalies[anomalies['anomaly_score'] >= 0.7]\n",
        "    \n",
        "    if len(high_score_anomalies) > 0:\n",
        "        print(f\"Anomalias com score alto (>= 0.7): {len(high_score_anomalies)}\")\n",
        "        print(\"\\nPara enviar alertas, descomente as linhas abaixo e configure:\")\n",
        "        print(\"  - Discord webhook em src/config.py\")\n",
        "        print(\"  - Teams webhook em src/config.py\")\n",
        "        print(\"  - Email SMTP em src/config.py\")\n",
        "        \n",
        "        # Exemplo (descomentar para usar):\n",
        "        # results = send_anomaly_alerts(\n",
        "        #     high_score_anomalies,\n",
        "        #     produto_id=\"TODOS\",\n",
        "        #     min_score=0.7,\n",
        "        #     send_discord=False,  # Configure webhook primeiro\n",
        "        #     send_teams=False,    # Configure webhook primeiro\n",
        "        #     send_email=False     # Configure SMTP primeiro\n",
        "        # )\n",
        "        # print(f\"\\nResultados do envio: {results}\")\n",
        "    else:\n",
        "        print(\"Nenhuma anomalia com score >= 0.7 encontrada.\")\n",
        "else:\n",
        "    print(\"Nenhuma anomalia detectada.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enviar alertas por produto (exemplo)\n",
        "# NOTA: Configure os webhooks/email em src/config.py antes de usar\n",
        "\n",
        "if len(anomalies) > 0:\n",
        "    print(\"Para enviar alertas por produto, descomente as linhas abaixo:\")\n",
        "    \n",
        "    # Exemplo (descomentar para usar):\n",
        "    # results_by_product = send_anomaly_alert_by_product(\n",
        "    #     anomalies,\n",
        "    #     produto_column=\"produto_id\",\n",
        "    #     min_score=0.7,\n",
        "    #     send_discord=False,\n",
        "    #     send_teams=False,\n",
        "    #     send_email=False\n",
        "    # )\n",
        "    # print(f\"\\nAlertas enviados para {len(results_by_product)} produtos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Salvar Resultados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar modelo de anomalias\n",
        "model_path = Path(\"outputs/models/isolation_forest_model.pkl.gz\")\n",
        "save_anomaly_model(anomaly_model, model_path, compress=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar todos os dados com flag de anomalia\n",
        "output_path_all = Path(\"outputs/anomalies_detected.csv\")\n",
        "save_processed(df_with_anomalies, output_path_all, format=\"csv\")\n",
        "print(f\"Dados completos salvos em: {output_path_all}\")\n",
        "\n",
        "# Salvar em Parquet (mais eficiente)\n",
        "output_path_parquet = Path(\"outputs/anomalies_detected.parquet\")\n",
        "save_processed(df_with_anomalies, output_path_parquet, format=\"parquet\", compress=True)\n",
        "print(f\"Dados completos salvos em Parquet: {output_path_parquet}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvar apenas as anomalias\n",
        "if len(anomalies) > 0:\n",
        "    output_path_anomalies = Path(\"outputs/anomalies_only.csv\")\n",
        "    save_processed(anomalies, output_path_anomalies, format=\"csv\")\n",
        "    print(f\"Apenas anomalias salvas em: {output_path_anomalies}\")\n",
        "    \n",
        "    # Salvar em Parquet\n",
        "    output_path_anomalies_parquet = Path(\"outputs/anomalies_only.parquet\")\n",
        "    save_processed(anomalies, output_path_anomalies_parquet, format=\"parquet\", compress=True)\n",
        "    print(f\"Apenas anomalias salvas em Parquet: {output_path_anomalies_parquet}\")\n",
        "else:\n",
        "    print(\"Nenhuma anomalia para salvar.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Carregar Modelo Salvo (Exemplo)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemplo de como carregar um modelo salvo\n",
        "# loaded_model = load_anomaly_model(Path(\"outputs/models/isolation_forest_model.pkl.gz\"))\n",
        "# print(\"Modelo carregado com sucesso!\")\n",
        "\n",
        "# Usar modelo carregado para detectar anomalias em novos dados\n",
        "# df_new = ...  # Seus novos dados\n",
        "# df_new_anomalies = detect_anomalies(loaded_model, df_new, feature_columns=feature_columns)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
